# 0.2b Explore etcd

> The cluster's brain - where ALL state is stored

---

## What is etcd?

etcd (pronounced "et-see-dee") is a distributed key-value database. It stores **everything** about your cluster:

```text
┌─────────────────────────────────────────────────────────────────┐
│                           etcd                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   Key-Value Store:                                              │
│                                                                 │
│   /registry/pods/default/nginx-abc123      → {pod spec JSON}    │
│   /registry/pods/default/redis-xyz789      → {pod spec JSON}    │
│   /registry/services/default/my-service    → {service JSON}     │
│   /registry/deployments/default/my-app     → {deployment JSON}  │
│   /registry/nodes/playground-worker        → {node info JSON}   │
│   /registry/secrets/default/db-password    → {encrypted secret} │
│   /registry/configmaps/default/app-config  → {config JSON}      │
│                                                                 │
│   Everything you create with kubectl ends up here.              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Why etcd Matters

```text
┌─────────────────────────────────────────────────────────────────┐
│                    WITHOUT etcd                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   • No record of what should be running                         │
│   • No way to know cluster state                                │
│   • API Server can't answer any questions                       │
│   • Controllers don't know what to manage                       │
│   • Scheduler doesn't know what needs scheduling                │
│                                                                 │
│   etcd dies = cluster is brain dead                             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Hands-On: Find etcd

### Step 1: See the etcd Pod

```bash
# Find etcd
kubectl get pods -n kube-system | grep etcd
```

Output:
```text
etcd-playground-control-plane   1/1     Running   ...
```

### Step 2: Get etcd Details

```bash
# Describe it
kubectl describe pod etcd-playground-control-plane -n kube-system
```

Notice:
- Runs only on control-plane
- Uses client certificates for security
- Data stored in `/var/lib/etcd`

---

## Hands-On: Look Inside etcd

Let's actually see what's stored in etcd!

### Step 1: Access etcd

```bash
# Exec into the control-plane node
podman exec -it playground-control-plane bash
```

### Step 2: Use etcdctl

Inside the container:

```bash
# Set up etcdctl with certificates
export ETCDCTL_API=3
export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt
export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt
export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key

# Check etcd health
etcdctl endpoint health

# List all keys (this is EVERYTHING in your cluster!)
etcdctl get / --prefix --keys-only | head -50
```

You'll see keys like:
```text
/registry/pods/default/...
/registry/services/default/...
/registry/deployments/...
/registry/namespaces/...
```

### Step 3: Read a Specific Key

```bash
# First, create a pod (exit and run this outside)
exit
kubectl run etcd-test --image=nginx:alpine

# Go back in
podman exec -it playground-control-plane bash

# Set up etcdctl again
export ETCDCTL_API=3
export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt
export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt
export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key

# Read the pod data
etcdctl get /registry/pods/default/etcd-test
```

You'll see the raw pod data (it's encoded, but you can see the structure).

### Step 4: Watch etcd Changes

In one terminal (inside control-plane):
```bash
# Watch for changes
etcdctl watch / --prefix
```

In another terminal:
```bash
# Create something
kubectl run watch-test --image=nginx:alpine

# Delete it
kubectl delete pod watch-test
```

Watch the first terminal - you'll see etcd keys being created and deleted!

```bash
# Exit when done
exit
```

---

## Hands-On: etcd Metrics

```bash
# Get etcd pod metrics
kubectl top pod etcd-playground-control-plane -n kube-system

# If metrics-server isn't installed, check via logs
kubectl logs etcd-playground-control-plane -n kube-system | tail -20
```

---

## Hands-On: Count Cluster Objects

Let's count everything stored in etcd:

```bash
# Count pods across all namespaces
kubectl get pods -A --no-headers | wc -l

# Count all resources
kubectl get all -A --no-headers | wc -l

# More detailed breakdown
echo "Pods: $(kubectl get pods -A --no-headers 2>/dev/null | wc -l)"
echo "Services: $(kubectl get services -A --no-headers 2>/dev/null | wc -l)"
echo "Deployments: $(kubectl get deployments -A --no-headers 2>/dev/null | wc -l)"
echo "ConfigMaps: $(kubectl get configmaps -A --no-headers 2>/dev/null | wc -l)"
echo "Secrets: $(kubectl get secrets -A --no-headers 2>/dev/null | wc -l)"
```

All of this is stored in etcd!

---

## Hands-On: etcd Data Size

```bash
# Check etcd database size
podman exec playground-control-plane du -sh /var/lib/etcd/

# More detailed
podman exec playground-control-plane ls -la /var/lib/etcd/member/snap/
```

For a learning cluster, it's usually just a few MB. Production clusters with thousands of resources can have GBs.

---

## Experiment: What Happens When You Create a Resource?

Let's trace the flow from kubectl to etcd:

### Step 1: Watch etcd

Terminal 1 - Watch etcd:
```bash
podman exec -it playground-control-plane bash -c '
  export ETCDCTL_API=3
  export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt
  export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt
  export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key
  etcdctl watch /registry/pods/default --prefix
'
```

### Step 2: Create a Pod

Terminal 2 - Create pod:
```bash
kubectl run trace-test --image=nginx:alpine
```

### Step 3: Observe

In Terminal 1, you'll see:
```text
PUT
/registry/pods/default/trace-test
...pod data...
```

The API server wrote to etcd!

### Step 4: Delete and Watch

```bash
kubectl delete pod trace-test
```

Terminal 1 shows:
```text
DELETE
/registry/pods/default/trace-test
```

---

## Experiment: etcd is the Source of Truth

What happens if kubectl and etcd disagree?

```bash
# Create a deployment
kubectl create deployment etcd-truth --image=nginx:alpine --replicas=2

# Check pods
kubectl get pods

# The deployment controller reads from etcd:
# "Desired: 2 pods, Current: 2 pods" ✓

# If a pod dies, the controller sees the difference and creates a new one
kubectl delete pod -l app=etcd-truth --wait=false

# Immediately check
kubectl get pods -w
```

The controller sees the discrepancy in etcd and fixes it automatically!

```bash
# Cleanup
kubectl delete deployment etcd-truth
```

---

## Hands-On: etcd Backup (Concept)

In production, etcd backup is critical. Here's how it works:

```bash
# Inside control-plane container:
podman exec -it playground-control-plane bash

export ETCDCTL_API=3
export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt
export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt
export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key

# Create a snapshot backup
etcdctl snapshot save /tmp/etcd-backup.db

# Verify the backup
etcdctl snapshot status /tmp/etcd-backup.db --write-out=table

exit
```

Output:
```text
+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| abc123.. |    12345 |        456 |     2.1 MB |
+----------+----------+------------+------------+
```

**In production:** You'd copy this backup to external storage regularly.

---

## Why etcd is Distributed

In production, etcd runs on multiple nodes for reliability:

```text
┌─────────────────────────────────────────────────────────────────┐
│              PRODUCTION etcd CLUSTER                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   ┌─────────────┐  ┌─────────────┐  ┌─────────────┐            │
│   │   etcd-1    │  │   etcd-2    │  │   etcd-3    │            │
│   │  (leader)   │──│  (follower) │──│  (follower) │            │
│   └─────────────┘  └─────────────┘  └─────────────┘            │
│         │                │                │                     │
│         └────────────────┴────────────────┘                     │
│                          │                                      │
│                 Raft consensus protocol                         │
│                 (majority must agree)                           │
│                                                                 │
│   If 1 node dies: Cluster continues (2/3 = majority)           │
│   If 2 nodes die: Cluster stops (1/3 = no majority)            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

Our kind cluster has only 1 etcd (single point of failure) - fine for learning, not for production.

---

## Key Takeaways

1. **etcd is the database** - Every Kubernetes object is stored here
2. **Key-value store** - Simple `/registry/pods/namespace/name` = `{JSON}`
3. **Only API Server talks to it** - Never access directly in production
4. **Source of truth** - If it's not in etcd, Kubernetes doesn't know about it
5. **Backup is critical** - Lose etcd = lose your cluster state
6. **Distributed in production** - 3 or 5 nodes for high availability

---

## Quick Reference

```bash
# Find etcd pod
kubectl get pods -n kube-system | grep etcd

# Describe etcd
kubectl describe pod etcd-playground-control-plane -n kube-system

# Access etcd inside control-plane
podman exec -it playground-control-plane bash

# Inside: set up etcdctl
export ETCDCTL_API=3
export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt
export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt
export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key

# Check health
etcdctl endpoint health

# List all keys
etcdctl get / --prefix --keys-only

# Watch changes
etcdctl watch / --prefix

# Backup
etcdctl snapshot save /tmp/backup.db
```

---

**Next:** [0.2c Explore Scheduler](./0.2c_explore_scheduler.md)
