# 0.2c Explore the Scheduler

> The matchmaker - decides WHERE your pods run

---

## What is the Scheduler?

The Scheduler (`kube-scheduler`) watches for new pods with no node assigned, then picks the best node for them.

```text
┌─────────────────────────────────────────────────────────────────┐
│                      SCHEDULER                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   New pod created (no node assigned)                            │
│            │                                                    │
│            ▼                                                    │
│   ┌─────────────────────────────────────────┐                   │
│   │  FILTERING: Which nodes CAN run this?   │                   │
│   │                                         │                   │
│   │  • Enough CPU/Memory?                   │                   │
│   │  • Matches nodeSelector?                │                   │
│   │  • Tolerates node taints?               │                   │
│   │  • Port available?                      │                   │
│   └─────────────────────────────────────────┘                   │
│            │                                                    │
│            ▼                                                    │
│   ┌─────────────────────────────────────────┐                   │
│   │  SCORING: Which node is BEST?           │                   │
│   │                                         │                   │
│   │  • Most resources available             │                   │
│   │  • Least number of pods                 │                   │
│   │  • Data locality                        │                   │
│   │  • Affinity rules                       │                   │
│   └─────────────────────────────────────────┘                   │
│            │                                                    │
│            ▼                                                    │
│   Winner: "playground-worker2"                                  │
│   Scheduler updates pod: nodeName=playground-worker2            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Hands-On: Find the Scheduler

```bash
# Find the scheduler pod
kubectl get pods -n kube-system | grep scheduler

# Describe it
kubectl describe pod kube-scheduler-playground-control-plane -n kube-system
```

---

## Hands-On: Watch Scheduling Happen

### Step 1: Watch Events

```bash
# Watch cluster events in real-time
kubectl get events -w
```

### Step 2: Create a Pod (Different Terminal)

```bash
# Create a pod
kubectl run sched-test --image=nginx:alpine
```

### Step 3: Observe

In the events window, you'll see:
```text
0s    Normal   Scheduled   pod/sched-test   Successfully assigned default/sched-test to playground-worker
0s    Normal   Pulling     pod/sched-test   Pulling image "nginx:alpine"
0s    Normal   Pulled      pod/sched-test   Successfully pulled image
0s    Normal   Created     pod/sched-test   Created container nginx
0s    Normal   Started     pod/sched-test   Started container nginx
```

**"Successfully assigned default/sched-test to playground-worker"** - That's the Scheduler!

```bash
# Cleanup
kubectl delete pod sched-test
```

---

## Hands-On: See Which Node a Pod Runs On

```bash
# Create multiple pods
kubectl run sched-a --image=nginx:alpine
kubectl run sched-b --image=nginx:alpine
kubectl run sched-c --image=nginx:alpine

# See where they were scheduled
kubectl get pods -o wide
```

Output:
```text
NAME      READY   STATUS    IP           NODE
sched-a   1/1     Running   10.244.1.5   playground-worker
sched-b   1/1     Running   10.244.2.3   playground-worker2
sched-c   1/1     Running   10.244.1.6   playground-worker
```

The scheduler distributed pods across nodes!

```bash
# Cleanup
kubectl delete pod sched-a sched-b sched-c
```

---

## Hands-On: Force a Pod to a Specific Node

### Method 1: nodeName (Bypass Scheduler)

```bash
# Get your node names
kubectl get nodes

# Create a pod that MUST run on a specific node
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: forced-node
spec:
  nodeName: playground-worker2   # Directly set node, bypasses scheduler
  containers:
  - name: nginx
    image: nginx:alpine
EOF

# Verify
kubectl get pod forced-node -o wide
```

The `nodeName` field bypasses the scheduler entirely!

```bash
kubectl delete pod forced-node
```

### Method 2: nodeSelector (Scheduler Respects This)

First, let's label a node:

```bash
# Add a label to worker2
kubectl label nodes playground-worker2 disk=ssd

# Verify
kubectl get nodes --show-labels | grep disk
```

Now create a pod that requires this label:

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: ssd-pod
spec:
  nodeSelector:
    disk: ssd          # Only schedule on nodes with disk=ssd
  containers:
  - name: nginx
    image: nginx:alpine
EOF

# See where it landed
kubectl get pod ssd-pod -o wide
# Should be on playground-worker2!
```

```bash
# Cleanup
kubectl delete pod ssd-pod
kubectl label nodes playground-worker2 disk-
```

---

## Hands-On: What If No Node Matches?

```bash
# Create a pod requiring a non-existent label
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: impossible-pod
spec:
  nodeSelector:
    special-hardware: gpu    # No node has this!
  containers:
  - name: nginx
    image: nginx:alpine
EOF

# Check status
kubectl get pod impossible-pod
```

Output:
```text
NAME             READY   STATUS    RESTARTS   AGE
impossible-pod   0/1     Pending   0          30s
```

The pod is **Pending** forever!

```bash
# See why
kubectl describe pod impossible-pod | grep -A5 Events
```

```text
Events:
  Type     Reason            Message
  ----     ------            -------
  Warning  FailedScheduling  0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector
```

The scheduler tried but found no valid nodes.

```bash
kubectl delete pod impossible-pod
```

---

## Hands-On: Resource Requests and Scheduling

The scheduler considers resource requests when picking nodes.

### Step 1: Check Node Capacity

```bash
# See how much each node can offer
kubectl describe nodes | grep -A5 "Allocatable:"
```

### Step 2: Create a Pod with Resource Requests

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: resource-pod
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    resources:
      requests:
        memory: "256Mi"
        cpu: "250m"
EOF

kubectl get pod resource-pod -o wide
```

### Step 3: Create a "Greedy" Pod

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: greedy-pod
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    resources:
      requests:
        memory: "10Gi"     # Probably more than any node has!
        cpu: "10"
EOF

kubectl get pod greedy-pod
```

This will stay **Pending** - no node has enough resources!

```bash
kubectl describe pod greedy-pod | grep -A3 Events
```

```text
Warning  FailedScheduling  0/3 nodes are available: 3 Insufficient memory, 3 Insufficient cpu
```

```bash
# Cleanup
kubectl delete pod resource-pod greedy-pod
```

---

## Hands-On: Watch the Scheduler Logs

```bash
# See scheduler decisions
kubectl logs kube-scheduler-playground-control-plane -n kube-system | tail -30

# Watch in real-time
kubectl logs -f kube-scheduler-playground-control-plane -n kube-system &

# Create a pod
kubectl run log-test --image=nginx:alpine

# Kill the log watcher
pkill -f "kubectl logs -f kube-scheduler"

kubectl delete pod log-test
```

---

## Hands-On: Scheduler Dry-Run

Want to see where a pod WOULD be scheduled without actually creating it?

```bash
# Create a pod YAML
cat <<EOF > /tmp/test-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: dry-run-pod
spec:
  containers:
  - name: nginx
    image: nginx:alpine
EOF

# Dry run - shows what would happen
kubectl apply -f /tmp/test-pod.yaml --dry-run=server -o yaml | grep nodeName
```

Note: The scheduler doesn't run during dry-run, so `nodeName` won't be set. But validation happens!

---

## Experiment: What If the Scheduler Dies?

The scheduler is a static pod that restarts automatically, but let's see what happens conceptually:

```bash
# Check existing pods scheduled
kubectl get pods -o wide

# If scheduler was down:
# - Existing pods keep running (no effect)
# - NEW pods would stay Pending with no node assigned

# This is why in production you run multiple schedulers
```

---

## Advanced: Pod Priority and Preemption

Higher priority pods can evict lower priority pods:

```bash
# Create a PriorityClass
cat <<EOF | kubectl apply -f -
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "High priority pods"
EOF

# Create a high-priority pod
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: vip-pod
spec:
  priorityClassName: high-priority
  containers:
  - name: nginx
    image: nginx:alpine
EOF

kubectl get pod vip-pod -o wide
```

In a resource-constrained cluster, this pod could evict lower priority pods!

```bash
# Cleanup
kubectl delete pod vip-pod
kubectl delete priorityclass high-priority
```

---

## Key Takeaways

1. **Scheduler picks nodes for new pods** - It doesn't manage existing pods
2. **Two-phase process** - Filter (can it fit?) → Score (which is best?)
3. **Resource-aware** - Considers CPU, memory, ports
4. **Constraint-aware** - Respects nodeSelector, affinity, taints
5. **Pending = can't schedule** - Check events for why
6. **Only assigns, doesn't start** - kubelet actually starts the pod

---

## Quick Reference

```bash
# Find scheduler
kubectl get pods -n kube-system | grep scheduler

# Watch scheduling events
kubectl get events -w

# See pod node assignment
kubectl get pods -o wide

# Force node (bypass scheduler)
spec.nodeName: node-name

# Label-based scheduling
kubectl label nodes <node> key=value
spec.nodeSelector:
  key: value

# Check why Pending
kubectl describe pod <pod-name>

# View scheduler logs
kubectl logs kube-scheduler-playground-control-plane -n kube-system
```

---

**Next:** [0.2d Explore Controller Manager](./0.2d_explore_controller_manager.md)
