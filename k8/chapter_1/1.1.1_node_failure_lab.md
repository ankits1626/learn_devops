# Lab 1.1.1: Understanding Node Failure & Pod Rescheduling

> See what happens when nodes die — and why bare Pods are dangerous

---

## Prerequisites

- Podman Desktop running (`podman machine start`)
- kind installed (`brew install kind`)
- A multi-node kind cluster

---

## The Hierarchy

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        KUBERNETES CLUSTER                               │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐    │
│  │                      CONTROL PLANE                              │    │
│  │   Watches all nodes, reschedules pods if node dies              │    │
│  └─────────────────────────────────────────────────────────────────┘    │
│                              │                                          │
│          ┌───────────────────┼───────────────────┐                      │
│          ▼                   ▼                   ▼                      │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐               │
│  │    NODE 1    │    │    NODE 2    │    │    NODE 3    │               │
│  │  (worker)    │    │  (worker)    │    │  (worker)    │               │
│  ├──────────────┤    ├──────────────┤    ├──────────────┤               │
│  │    Pods      │    │    Pods      │    │    Pods      │               │
│  │  ┌────────┐  │    │  ┌────────┐  │    │  ┌────────┐  │               │
│  │  │Container│ │    │  │Container│ │    │  │Container│ │               │
│  │  └────────┘  │    │  └────────┘  │    │  └────────┘  │               │
│  └──────────────┘    └──────────────┘    └──────────────┘               │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

**Key relationships:**
- **Cluster** contains multiple **Nodes**
- **Node** is a machine (physical/virtual) that runs **Pods**
- **Pod** wraps one or more **Containers**
- **Container** is your actual application

---

## Why Nodes Fail

| Cause | Example |
|-------|---------|
| Hardware failure | Disk dies, RAM fails, power loss |
| Network issues | Node loses connectivity to control plane |
| Resource exhaustion | Out of memory, disk full |
| OS crash | Kernel panic, system hang |
| Cloud provider | Spot instance terminated, AZ outage |

---

## Lab Setup: Create a 3-Node Cluster

### Step 1: Create cluster config

```bash
cat << 'EOF' > /tmp/kind-3node.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
EOF
```

### Step 2: Create the cluster with Podman

```bash
# Ensure podman is running
podman machine start

# Create cluster using podman as provider
KIND_EXPERIMENTAL_PROVIDER=podman kind create cluster \
  --name node-failure-lab \
  --config /tmp/kind-3node.yaml
```

### Step 3: Verify nodes

```bash
kubectl get nodes

# Expected output:
# NAME                            STATUS   ROLES           AGE   VERSION
# node-failure-lab-control-plane  Ready    control-plane   1m    v1.31.0
# node-failure-lab-worker         Ready    <none>          1m    v1.31.0
# node-failure-lab-worker2        Ready    <none>          1m    v1.31.0
```

---

## Experiment 1: Bare Pod Dies Forever

### Step 1: Create a bare pod

```bash
cat << 'EOF' | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: bare-nginx
  labels:
    app: bare-nginx
spec:
  containers:
    - name: nginx
      image: nginx:1.25
EOF
```

### Step 2: Check which node it's on

```bash
kubectl get pod bare-nginx -o wide

# Note the NODE column - remember this!
# Example output:
# NAME         READY   STATUS    NODE
# bare-nginx   1/1     Running   node-failure-lab-worker2
```

### Step 3: Kill that node (simulate failure)

```bash
# List kind containers (nodes) in podman
podman ps --filter "name=node-failure-lab"

# Stop the worker node where the pod is running
# Replace with actual node name from step 2
podman stop node-failure-lab-worker2
```

### Step 4: Watch what happens

```bash
# Watch node status
kubectl get nodes -w

# After ~40 seconds, you'll see:
# node-failure-lab-worker2   NotReady   <none>   5m   v1.31.0

# Check the pod
kubectl get pod bare-nginx -o wide

# Pod is stuck in "Terminating" or shows the dead node
# IT WILL NEVER COME BACK ON ITS OWN!
```

### Step 5: Bring node back and clean up

```bash
# Restart the node
podman start node-failure-lab-worker2

# Wait for node to be Ready again
kubectl get nodes -w

# Delete the stuck pod (may need --force)
kubectl delete pod bare-nginx --force --grace-period=0
```

---

## Experiment 2: Deployment Survives Node Failure

### Step 1: Create a deployment with 3 replicas

```bash
cat << 'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resilient-nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: resilient-nginx
  template:
    metadata:
      labels:
        app: resilient-nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.25
EOF
```

### Step 2: Verify pods are spread across nodes

```bash
kubectl get pods -l app=resilient-nginx -o wide

# Expected: pods on different nodes
# NAME                              READY   NODE
# resilient-nginx-xxxxx-abc12       1/1     node-failure-lab-worker
# resilient-nginx-xxxxx-def34       1/1     node-failure-lab-worker2
# resilient-nginx-xxxxx-ghi56       1/1     node-failure-lab-worker
```

### Step 3: Kill a worker node

```bash
# Stop worker2 (or whichever has a pod)
podman stop node-failure-lab-worker2
```

### Step 4: Watch the magic happen

```bash
# Watch pods in real-time
kubectl get pods -l app=resilient-nginx -o wide -w

# Timeline:
# 0s    - All 3 pods running
# 40s   - Node marked NotReady
# ~5min - Pod on dead node marked Terminating
#       - NEW pod created on healthy node!
#       - Still have 3 running pods!
```

**Note:** The default pod eviction timeout is 5 minutes. For faster testing, you can watch the node status change immediately.

### Step 5: Verify recovery

```bash
# After Kubernetes reschedules
kubectl get pods -l app=resilient-nginx -o wide

# You should see 3 RUNNING pods, all on healthy nodes
# The pod from the dead node was replaced!
```

### Step 6: Restore and cleanup

```bash
# Bring node back
podman start node-failure-lab-worker2

# Wait for Ready
kubectl get nodes -w

# Clean up
kubectl delete deployment resilient-nginx
```

---

## What Happened: Visual Timeline

```
BARE POD:
─────────────────────────────────────────────────────────────
Time    Node Status      Pod Status       Recovery?
─────────────────────────────────────────────────────────────
0s      All Ready        Running          -
40s     Worker2 NotReady Running          -
5min    Worker2 NotReady Terminating      NO! Pod is gone
        Node comes back  Pod deleted      Must recreate manually
─────────────────────────────────────────────────────────────

DEPLOYMENT:
─────────────────────────────────────────────────────────────
Time    Node Status      Pods Running     Recovery?
─────────────────────────────────────────────────────────────
0s      All Ready        3/3              -
40s     Worker2 NotReady 3/3              -
5min    Worker2 NotReady 2/3 → 3/3        YES! Auto-rescheduled
        New pod created on healthy node!
─────────────────────────────────────────────────────────────
```

---

## Key Takeaways

| Scenario | What Happens | Use Case |
|----------|--------------|----------|
| **Bare Pod** | Dies with node, never comes back | Testing, one-off jobs |
| **Deployment** | Automatically rescheduled to healthy node | Production workloads |

### The Golden Rule

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│   NEVER use bare Pods in production.                        │
│   ALWAYS use Deployments (or StatefulSets for databases).   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Cleanup

```bash
# Delete the lab cluster entirely
KIND_EXPERIMENTAL_PROVIDER=podman kind delete cluster --name node-failure-lab
```

---

## Quick Reference: Podman + Kind Commands

```bash
# Start podman machine
podman machine start

# List kind clusters
kind get clusters

# List kind nodes as podman containers
podman ps --filter "name=node-failure-lab"

# Stop a node (simulate failure)
podman stop <node-container-name>

# Start a node (recover)
podman start <node-container-name>

# View node logs
podman logs <node-container-name>

# Delete cluster
KIND_EXPERIMENTAL_PROVIDER=podman kind delete cluster --name <cluster-name>
```

---

## Next Steps

Now that you understand:
- Nodes can fail at any time
- Bare pods don't recover
- Deployments provide self-healing

Continue to [1.2 Pod YAML Anatomy](1.2_pod_yaml_anatomy.md) to learn how to write proper pod manifests.

---

*Lab 1.1.1 of Kubernetes 101: From Zero to Production*
